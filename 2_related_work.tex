\section{Person Detection Methods Using 2D LiDAR}

In research using 2D LiDAR, a common approach involves clustering the obtained point cloud data of horizontal cross-sections, detecting them as cross-sectional shapes of legs or torsos, and then performing time-series tracking using methods such as Kalman filters.

Arras et al. \cite{2D LiDAR} proposed a method to robustly detect people even in complex environments using planar scan data obtained from a 2D Laser Range Finder (LRF). Many conventional approaches used pre-defined features to detect and track human legs, but these had the problem of relying on manual design and tuning. In contrast, Arras et al. constructed a high-precision detector adapted to the environment by using an approach that selects and integrates effective features from a large number of candidates through supervised learning.

In their proposed method, the point cloud data obtained from the LRF is first segmented using a "jump distance condition," which divides the data at points where the distance between adjacent beams exceeds a certain threshold. Each obtained segment is treated as a leg candidate, and 14 types of scalar features representing its shape and statistical properties are calculated. Specifically, in addition to static geometric features—such as the number of points composing the segment, standard deviation, mean deviation from the median (which is robust against outliers), jump distance to adjacent segments, segment width, linearity and circularity (fitness to lines and circles using the least squares method), radius of the fitted circle, boundary length, boundary regularity, mean curvature, and mean angular difference measuring convexity—dynamic features such as the mean speed calculated from the difference between consecutive scans are defined.

AdaBoost is adopted for the learning algorithm. By combining these simple features (weak classifiers), a strong classifier is constructed to distinguish between segments corresponding to human legs and those that are not. By using AdaBoost, it becomes possible to automatically learn the most informative features and optimal thresholds from the data so as to maximize classification performance.

Experiments were conducted in corridors and cluttered office environments where both stationary and moving people were present. The performance evaluation of the learned classifier showed that it could detect people with a high accuracy rate exceeding 90\%, even in complex environments with many pieces of furniture and objects. In particular, compared to heuristic methods based on geometric rules often used in conventional research (such as setting fixed thresholds for leg width or jump distance), the proposed method significantly reduced the false detection rate, demonstrating its superiority. Furthermore, it was confirmed that high accuracy was maintained even when applying a classifier trained in one environment to an unknown environment not included in the training data, indicating that the learned model possesses high versatility.

Moreover, through the analysis of the learned classifier, important insights regarding effective features for person detection were obtained. As a result of analyzing the weights of the features selected by AdaBoost, the features contributing most to the classification were not the simple leg width or circularity, but the "radius" of the fitted circle and the "convexity (mean angular difference)" of the segment. This suggests that while the shape of legs in 2D scan data takes various forms and is not necessarily a clean arc or straight line, the sense of size and the degree of outward swelling are the most robust indicators. In addition, experimental results from cases where the dynamic feature "movement speed" was added showed that the improvement in accuracy was slight compared to cases using only static geometric features. From this, Arras et al. concluded that motion information is not essential for robust detection including stationary people, and that sufficiently high-precision detection is possible with just a combination of appropriate geometric features. However, they did not mention re-detection or recovery of tracking after occlusion.



\section{Person Following Methods Using 3D LiDAR}

In research using 3D LiDAR, a common pipeline involves removing ground point clouds from 3D spatial data, extracting point cloud clusters that are candidates for people using Euclidean clustering or similar methods, and then tracking them using Kalman filters or particle filters. Yan et al. \cite{3D LiDAR} proposed a framework where a mobile robot equipped with a 3D LiDAR learns a person detector online while adapting to the driving environment. In person detection using 3D LiDAR, changes in point cloud density due to distance and the human cost involved in creating training data (annotation effort) have been major issues. In response to this, Yan et al. developed a method to automatically retrain a classifier (SVM), which is initialized with a small amount of labeled data, using data obtained during the robot's operation.

The core of the proposed method is an adaptive clustering technique and a training data generation mechanism called "P-N Experts." First, in the segmentation (clustering) of point clouds, to handle the point cloud density becoming sparser with distance from the LiDAR, adaptive clustering is adopted. This sets different distance thresholds for concentric zones centered on the sensor. This achieves higher precision segmentation than conventional methods based on Euclidean distance or depth image-based methods, in both dense point clouds at close range and sparse point clouds at long range.

For person classification, an SVM classifier using geometric features is employed. However, to specifically improve detection performance at long distances, "Slice features" are newly introduced. This involves dividing the point cloud into 10 vertical slices and describing the width and depth of each slice. This enables robust feature extraction even for distant people with few data points.

In the online learning process, the results of multi-target tracking (UKF) are used to automatically correct the errors of the classifier. Specifically, the "P-expert" uses the continuity of tracking trajectories to pick up missed detections (False Negatives) and adds them as positive examples, while the "N-expert" identifies false detections (False Positives) such as stationary objects and adds them as negative examples. By periodically retraining the classifier using the training data automatically generated in this way, it is possible to adapt to the characteristics of people and backgrounds specific to the environment without additional manual annotation.

Results of evaluation experiments using a dataset acquired in a large-scale indoor environment (L-CAS 3D Point Cloud People Dataset) confirmed that the proposed adaptive clustering showed higher segmentation accuracy compared to conventional methods. It was also confirmed that the classifier updated by online learning achieved a higher F-score (the harmonic mean of precision and recall) than a classifier trained manually offline. Yan et al.'s research is an important achievement showing the effectiveness of geometric features in person detection using 3D LiDAR and the utility of self-supervised learning by incorporating tracking information into the feedback loop. However, they did not mention re-detection or identification after occlusion.



\section{Occlusion-Aware Person Following Methods Using Monocular Cameras}

Person following systems using cameras (RGB images) are characterized by their ability to utilize rich optical information (Photometric features) such as the target's color and texture. In early studies, detection methods using color histograms or HOG (Histogram of Oriented Gradients) features were mainstream, but in recent years, methods combining Deep Learning and tracking algorithms have been widely researched. Additionally, approaches that incorporate online learning to adapt to the environment and target, rather than using pre-trained models as is, have also been proposed. Koide et al. \cite{Monocular} proposed a mobile robot person following system using only a "monocular camera," which is cheaper and easier to introduce compared to LiDAR or RGB-D cameras.

In person following using a monocular camera, there are mainly two technical challenges. The first challenge is "how to accurately estimate the target's 3D position from an image where depth information cannot be obtained." The second challenge is "how to realize robust tracking in real-time on a robot with limited computational resources, despite lighting variations and pose changes." Conventionally, methods using HOG features or color histograms with low computational cost (such as KCF) were fast but vulnerable to lighting changes and blending into the background. On the other hand, methods using deep learning (CNN) have high discrimination capability but extremely high computational load, making real-time operation on mobile robots equipped with embedded GPUs difficult.

To address these issues, Koide et al. attempted to solve them by integrating the following two approaches.

First, for the challenge of 3D position estimation, they introduced a tracking framework combining deep learning-based skeleton detection (OpenPose) and geometric constraints. Specifically, focusing on the positions of the "ankles" and "neck" of the person detected in the image, they combined the assumption that these exist on the Ground Plane with height estimation by a Kalman filter (UKF). This achieved high-precision position estimation in the robot coordinate system without using distance sensors. This method has the advantage of being able to continue tracking using the neck position information even when the feet are hidden by obstacles (occlusion).

Second, for the trade-off between computational cost and robustness, they proposed a method called "On-line Deep Feature Selection (ODFS)." Instead of using all the vast feature maps (Convolutional Channel Features: CCF) obtained from a VGG-16 model pre-trained on ImageNet, this method dynamically selects only the features most effective for separating the "target" from the "background" in the current frame using online boosting. This allows for adaptation, such as prioritizing color features when lighting conditions change or edge features when the shape is distinctive. This achieved real-time processing of 20fps on average on an embedded computer (NVIDIA Jetson TX2) while utilizing the expressive power of deep learning.

In evaluation experiments, verification was conducted in indoor and outdoor scenarios involving lighting changes, complex backgrounds, target rotation, and occlusion. As a result, the proposed method recorded the highest tracking success rate and accuracy compared to existing trackers like KCF and TLD. It demonstrated that robust person following in real environments is possible through the efficient use of deep learning, even with the limited sensor configuration of a monocular camera. On the other hand, this method using a monocular camera leaves the issue that direct measurement of depth information is fundamentally difficult and the field of view is limited. In the experiments, although it showed high following performance under specific conditions, conditions leading to mis-estimation or detection failure were reported, such as detection failure when the target was too close to the camera or reduced distance estimation accuracy at long distances, which are due to sensor characteristics.



\section{PointPillars}
Lang et al. \cite{PointPillars} proposed a new network architecture "PointPillars" aimed at balancing inference speed and detection accuracy in 3D object detection tasks, which are extremely important in autonomous driving and robotics.

Conventionally, for 3D object detection using point cloud data, there were methods that voxelized the point cloud and applied 3D Convolutional Neural Networks (3D CNN), or methods that projected the point cloud onto a Bird's Eye View (BEV) and applied 2D CNN. The former boasted high accuracy but had very high computational costs (e.g., VoxelNet), while the latter was fast but had the problem of losing 3D shape information, resulting in inferior accuracy. Also, methods that process point clouds directly, like PointNet, had good computational efficiency but issues with scalability for large-scale point clouds. In contrast, PointPillars adopts a new encoder that organizes point clouds into vertical columns (Pillars) for processing. This realizes a fast and high-precision detection pipeline composed only of standard 2D convolutions, without using any 3D convolutions.

The architecture consists of three main parts: the Pillar Feature Net (PFN), the Backbone (2D CNN), and the Detection Head (SSD). First, the Pillar Feature Net (PFN) plays the role of converting raw point cloud data into a "Pseudo-image." The input point cloud is first divided into vertical "Pillars" based on a grid on the $xy$ plane. Points within each pillar are represented as 9-dimensional feature vectors containing coordinates $(x, y, z)$, reflection intensity $r$, offset from the geometric center $x_c, y_c, z_c$, and offset from the pillar center $x_p, y_p$. By applying a simplified PointNet (Linear layer, Batch Norm, ReLU) to this, the features of each point are mapped to a high-dimensional space. Furthermore, by performing Max Pooling for each pillar, a feature vector representing each pillar is extracted. Through this process, the point cloud data is converted into a 2D pseudo-image with the format $(C, H, W)$. This method differs from conventional voxel-based methods in that it does not perform vertical binning, which makes hyperparameter tuning easier and allows for efficient handling of sparse data structures. Second, the Backbone network takes the generated pseudo-image as input and performs high-level feature extraction using 2D CNN. This backbone consists of down-sampling blocks that reduce the resolution of feature maps and blocks that up-sample and combine them to match the original resolution. By integrating features with different Receptive Fields, it acquires representations capable of handling objects of various sizes. Third, the Single Shot Detector (SSD), widely used in object detection, is adopted for the Detection Head. Based on the output feature maps from the backbone, it performs regression of 3D bounding boxes (position, size, angle) and class classification.

In evaluation experiments, PointPillars showed performance surpassing conventional methods boasting the highest accuracy (such as MV3D, VoxelNet, SECOND) on the KITTI 3D object detection benchmark (in both BEV and 3D detection). Notable is its inference speed, achieving extremely fast operation of 62 Hz on a desktop GPU. This is a result of eliminating 3D convolutions and using only optimized 2D convolution operations. Furthermore, it has been demonstrated that by using a Learned Encoder instead of the fixed encoders (manually designed features) used in conventional methods, significant accuracy improvement is achieved without sacrificing speed.

Although PointPillars proposed by Lang et al. is an excellent method in terms of the balance between inference speed and detection accuracy, several technical issues due to its structural characteristics have also become clear. First, false detections tend to occur in the identification of objects with similar geometric features. Since PointPillars compresses point clouds into vertical pillars for feature extraction, detailed information in the height direction is abstracted. Consequently, cases where long, thin vertical structures like poles or tree trunks are misidentified as pedestrians, or where pedestrians and cyclists are misclassified as each other, have been reported. Also, in the detection of passenger cars, cases of misdetecting similarly shaped vehicle classes like vans or trams (False Positives) have been confirmed.Next is the limitation of detection ability under poor observation conditions. Similar to other LiDAR-based methods, there is a tendency for missed detections (False Negatives) to occur when the target is partially occluded by other objects or is far from the sensor with extremely low point cloud density. Furthermore, the trade-off between spatial resolution and processing speed is also an issue. Ablation studies have shown that if the grid size is set large to prioritize inference speed, the detection performance for large objects like vehicles is maintained, but the detection accuracy for small targets like pedestrians and cyclists decreases. Therefore, to detect diverse targets simultaneously and at high speed in real environments, this trade-off needs to be carefully adjusted.



\section{ReID3D}

Guo et al. proposed a new framework "ReID3D" using only LiDAR sensors to overcome the problem of environmental dependency in conventional camera-based methods for the Person Re-identification (ReID) task, which plays an important role in public security and surveillance systems. Conventional ReID methods using RGB cameras have a fundamental issue of strongly depending on appearance information such as clothing color and texture. Therefore, visual information is missing at night or in low-light environments, and visual ambiguity increases in complex backgrounds, leading to a significant drop in identification accuracy. To address this problem, studies using Kinect or millimeter-wave radar as approaches utilizing depth information exist. However, Kinect is limited to indoor use with a narrow measurement range, and radar has low resolution, making detailed person identification difficult.

To solve these issues, Guo et al. focused on LiDAR, which is unaffected by lighting conditions and can acquire high-precision 3D structural information even at long distances. They aimed to realize ReID robust to clothing changes and darkness by extracting intrinsic features such as height, body shape, and gait from LiDAR point clouds. However, there was no precedent for research on ReID using LiDAR, and the lack of datasets necessary for learning was a major barrier. To solve this data shortage, the authors constructed the first real-world LiDAR ReID dataset "LReID" and a simulation dataset "LReID-sync." LReID is real data containing 320 IDs collected in diverse outdoor environments, while LReID-sync is synthetic data of 600 individuals generated using Unity3D.

The core of this method is a "Multi-task Pre-training" strategy to compensate for the sparsity of point cloud data and the lack of information due to a single viewpoint. First, when training the encoder using LReID-sync, two tasks are imposed: "Point Cloud Completion," which restores the complete overall shape from missing point clouds, and "SMPL parameter learning," which estimates the parameters of a human body model. This allows the encoder to acquire the geometric structure of the human body in advance, effectively supporting learning on real data. Also, a newly designed "Graph-based Complementary Enhancement Encoder (GCEE)" was adopted for the ReID network. GCEE uses a GCN that dynamically constructs graphs in the feature space as a backbone and introduces a Complementary Feature Extractor (CFE) with an "Eraser" module. The CFE eliminates information duplication between frames by erasing major features extracted from one frame before processing the next, enabling more comprehensive and highly discriminative feature extraction. Furthermore, a Transformer is used to integrate time-series information, efficiently aggregating dynamic features such as walking movements.

In evaluation experiments, ReID3D showed overwhelming robustness on the constructed LReID dataset compared to state-of-the-art camera-based methods (such as TCLNet). Especially in low-light environments where the accuracy of camera-based methods dropped significantly, ReID3D achieved a high performance of 93.3\% in Rank-1 accuracy (94.0\% overall), demonstrating the superiority of LiDAR which is independent of lighting conditions. In conclusion, ReID3D is a pioneering study that succeeded in extracting intrinsic 3D features of individuals from LiDAR point clouds by combining pre-training utilizing simulation data and GCEE specialized for point cloud characteristics, solving the challenge of person re-identification under adverse conditions which was difficult with conventional cameras alone.

While ReID3D shows excellent performance in low-light environments and in acquiring geometric structures, a challenge has been confirmed where its recognition accuracy is inferior to state-of-the-art video-based methods in bright (Normal light) environments with sufficient illumination. This is because video-based methods can maximize the use of rich appearance information (clothing color, pattern, texture, etc.) obtained in bright environments, whereas ReID3D, being a LiDAR-based method, relies only on shape information and reflection intensity and cannot use color information. Also, in the visualization analysis of the feature space, cases have been reported where ReID3D failed to identify specific pedestrians that would be easily identifiable by camera-based methods. These results suggest that LiDAR and cameras possess different modality characteristics and are in a complementary relationship with each other.



\section{Issues in Conventional Research and Positioning of This Study}

In person following by autonomous mobile robots, robustness when occlusion occurs remains an important issue to be solved. Methods using 2D LiDAR have low computational costs, but because the acquired information is limited to horizontal cross-sections, re-identification after occlusion or autonomous recovery from a lost state is difficult. On the other hand, methods using 3D LiDAR can acquire detailed spatial information, but most existing studies focus on the continuity of tracking, and methods for re-identification or recovery after tracking is completely interrupted by occlusion have not been sufficiently established. Additionally, while monocular cameras excel at re-identification using appearance information, they have issues such as the target moving out of the frame due to limited field of view and vulnerability to changes in lighting conditions.

Therefore, in this study, we propose a person following system using a robotic omnidirectional 3D LiDAR, "Livox Mid-360." By preventing the target from moving out of the frame with an omnidirectional field of view and utilizing point cloud data for re-detection and re-identification after occlusion, we aim to construct a system capable of autonomously continuing tracking even in complex environments.