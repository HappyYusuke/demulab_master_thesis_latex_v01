\section{Experimental Objectives}
The purpose of this study is to develop a human-following system using a 3D LiDAR. The main goal is to create a system that can re-identify the target person and continue tracking them even after they are hidden by obstacles (occlusion).

To verify the effectiveness of this system, we adopted a 3D LiDAR made by Livox as the sensor in our experiments. This sensor is designed for robots and has a unique scanning pattern. Therefore, we use PointPillars, which is an object detection network optimized for the characteristics of the point cloud generated by this sensor. We will quantitatively evaluate the accuracy of pedestrian detection using this setup.

In addition, we use a public dataset to evaluate the performance of re-identification. Specifically, we test whether the developed system can correctly identify the target person again after an occlusion occurs. Through these experiments, we aim to clarify the overall performance of the process, ranging from pedestrian detection to re-identification using 3D LiDAR. Finally, we demonstrate the usefulness of the human-following system we constructed.



\section{Experimental Methods}
In order to verify the effectiveness of the proposed system, we conduct two types of experiments.

The first experiment is a quantitative evaluation of the pedestrian detection accuracy of PointPillars. For this evaluation, the model has been optimized for the characteristics of the point cloud generated by the Livox 3D LiDAR. The second experiment is an evaluation of the tracking performance and re-identification performance of the entire human-following system that we constructed.

In addition, for the experimental equipment, we used a laptop PC equipped with an NVIDIA RTX 3070 (8GB) GPU.

\subsection{Comparative Experiment on Pedestrian Detection Performance}
In this experiment, we evaluate the pedestrian detection performance of PointPillars using the validation dataset of 4,500 frames constructed in Section 3.4. As evaluation metrics, we adopted BEV (Bird's Eye View) AP and 3D AP, which are widely used in object detection.

BEV AP is a metric calculated based on the area overlap with the ground truth. This is done by projecting the estimated 3D bounding box onto a 2D plane (bird's eye view). On the other hand, 3D AP is calculated based on the overlap ratio (IoU: Intersection over Union) of the 3D position and volume of the bounding box. In this experiment, the threshold for IoU was set to 0.5, following the general standard for pedestrian detection.

To set up the environment for the inference experiment, we adopted a specific data flow. The input data, which are binary point cloud files, are converted into ROS 2 topics and published. The detection model then subscribes to these topics. The publishing frequency of the topics is set to 10 Hz. In addition, to verify the effectiveness of our method, we used the standard model of the ros2\_tao\_pointpillars package as a baseline for comparison. This is an existing open-source implementation.

\subsection{Comparative Experiment on Tracking and Re-identification Performance}
In this experiment, we evaluate the tracking and re-identification performance of the entire human-following system we constructed. We used a public dataset for this evaluation. Specifically, we adopted TPT-BENCH \cite{TPT-Bench} as the evaluation dataset.

There are existing datasets related to robot perception, such as RoboSense \cite{RobSense} and JRDB \cite{JRDB-PanoTrack}. However, the main focus of this research is the task of continuously tracking a specific person (Target Person Tracking: TPT). Therefore, we selected TPT-BENCH because it is a dataset specialized for TPT and is optimal for this evaluation.

TPT-BENCH includes three types of sensor information: LiDAR point clouds, RGB images, and depth information. It is a large-scale dataset collected in diverse environments, such as indoor and outdoor settings, as well as day and night conditions.

We use three metrics for evaluation: Average Overlap (AO), F1-score, and Avg Max Recall (AMR).

AO represents the average value of the Intersection over Union (IoU) between the estimated bounding boxes and the ground truth. The F1-score is the harmonic mean of tracking Precision and Recall. This metric evaluates the balance between the accuracy and the completeness of the detection. AMR indicates the proportion of how successfully the system kept tracking the target correctly. It is a metric used to measure the robustness of the tracking.

\section{Experimental Results}
\subsection{Comparative Experiment on Pedestrian Detection Performance}
The experimental results for pedestrian detection are shown in Table \ref{Evaluation results of pedestrian detection.}. For ros2\_tao\_pointpillars, the BEV AP was 35.47\% and the 3D AP was 18.50\%. In contrast, our method achieved a BEV AP of 79.86\% and a 3D AP of 61.59\%. This means that our method significantly outperformed the conventional model in both metrics.

False detections were observed in both models. In ros2\_tao\_pointpillars, we observed cases where two pedestrians were detected as a single bounding box. On the other hand, our method was generally able to detect pedestrians successfully. However, there was a phenomenon where pedestrians were not detected when they stopped and stood upright.

\begin{table}[h]
  \centering
  \caption{Evaluation results of pedestrian detection.}
  \label{Evaluation results of pedestrian detection.}
  \begin{tabular}{ccc} \toprule
    Model & BEV AP (\%) & 3D AP (\%) \\ \midrule
    ros2\_tao\_pointpillars & 35.47 & 18.50 \\
    Ours & 79.86 & 61.59 \\ \bottomrule
  \end{tabular}
\end{table}



\section{Discussion}
From the experimental results, we confirmed that PointPillars, which was optimized for the point cloud characteristics of the Livox 3D LiDAR, demonstrated pedestrian detection performance that significantly exceeded that of the conventional model. We believe this is because the training using the unique dataset proposed in this study enabled feature extraction adapted to the sensor characteristics. Consequently, the model effectively overcame the domain gap in pedestrian detection.

However, we observed differences in the tendency of false detections between the two models. In ros2\_tao\_pointpillars, there were cases where pedestrians close to each other were incorrectly detected as a single bounding box. This may be influenced by the fact that the publicly available pre-trained model included annotations where two pedestrians were represented by a single bounding box.

On the other hand, in our proposed method, we observed a phenomenon where the model failed to detect pedestrians when they were stopped and standing upright. We consider that this is because the dataset contained a large number of point clouds where pedestrians were in motion.